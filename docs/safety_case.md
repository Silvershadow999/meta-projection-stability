# Safety Case — meta-projection-stability (CAE)

## 0. Purpose
This document provides a **structured Safety Case** using a **Claim–Argument–Evidence (CAE)** format.
It is focused on the repository’s role as a **safety-engineering / evaluation candidate**:

- structured telemetry
- explicit safety boundaries
- scenario-based comparison (baseline vs adversarial)
- reproducibility + provenance
- measurable outputs suitable for regression gating

## 1. Context and Intended Use
### Intended use (current)
- Local evaluation tool to run defined scenarios and emit telemetry to a JSONL log
- Produce a human-readable evaluation report for comparison across commits

### Not intended (current)
See `docs/non_goals.md` once present (planned).

## 2. Top-Level Claim (C0)
**C0:** The evaluation pipeline provides **credible, reproducible, and auditable safety telemetry** such that boundary behavior and key evaluation metrics can be compared across scenarios and commits.

### Acceptance Criteria for C0 (high-level)
- Every run emits RUN_START + RUN_END with provenance and scenario metadata
- The telemetry schema is versioned and stable (additive changes)
- Boundary triggers are explicit events (BOUNDARY) and are surfaced in reports
- Scenario manifests define deterministic inputs (id, seed, overrides)
- Reports include scenario comparison and highlight boundary differences

---

## 3. Subclaims, Arguments, and Evidence

### C1 — Telemetry is structured and schema-versioned
**Claim (C1):** Telemetry events follow a stable schema with explicit versioning.

**Argument (A1):**
- A single contract defines event types, severity, and event payload shape.
- Schema version enables controlled evolution and backward compatibility.

**Evidence (E1):**
- `src/meta_projection_stability/types.py`
  - `TELEMETRY_SCHEMA_VERSION`
  - `TelemetryEvent`, `BoundarySignal`, enums `EventType`, `Severity`

---

### C2 — Runs are reproducible and attributable (provenance)
**Claim (C2):** Each run can be attributed to code state and environment context, enabling reproducibility.

**Argument (A2):**
- Runs capture git commit hash + dirty state + Python/platform info in RUN_START payload.
- Scenario manifests provide deterministic seeds and configuration overrides.

**Evidence (E2):**
- `RunProvenance` in `src/meta_projection_stability/types.py`
- `scripts/eval_runner.py` (RUN_START payload includes provenance)
- `scenarios/*.json` (+ loader)

---

### C3 — Safety boundaries are explicit, logged, and reviewable
**Claim (C3):** Safety boundaries are explicit signals and produce auditable telemetry.

**Argument (A3):**
- Boundaries are represented as `BoundarySignal` and stored in `RunState`.
- Boundaries are emitted as dedicated `BOUNDARY` telemetry events.

**Evidence (E3):**
- `src/meta_projection_stability/state.py` (`RunState.boundaries`)
- `scripts/eval_runner.py` (emits `EventType.BOUNDARY`)
- `artifacts/results.jsonl` (contains boundary lines when triggered)
- `scripts/eval_report.py` (extracts boundary names into report)

---

### C4 — Scenario comparison supports adversarial reasoning
**Claim (C4):** The evaluation pipeline supports scenario-based comparison, including adversarial reasoning.

**Argument (A4):**
- Scenarios are defined as data and executed consistently.
- Reports compare latest run per scenario, enabling “baseline vs adversarial” inspection.

**Evidence (E4):**
- `src/meta_projection_stability/scenario_manifest.py` (loader + validation)
- `scripts/eval_report.py` (“Scenario Comparison” section)
- `scenarios/baseline.json` vs `scenarios/adversarial_min.json`

---

### C5 — Regression readiness (measurable outputs)
**Claim (C5):** Outputs are suitable for regression gating (future CI step), because they are machine-readable and stable.

**Argument (A5):**
- JSONL logs are append-only and structured.
- The report is derived, reproducible, and can be regenerated from JSONL.
- Metrics/boundaries are explicitly captured.

**Evidence (E5):**
- `artifacts/results.jsonl` format (generated by runner)
- `scripts/eval_report.py` derives `artifacts/eval_report.md`

---

## 4. Residual Risks and Limitations (current)
- Telemetry integrity is “best effort” without cryptographic signing.
- Runner currently uses a skeleton loop; model integration may introduce new failure modes.
- Report parsing is tolerant; strict schema validation is a planned hardening step.

## 5. Planned Hardening (next steps)
- Add a schema validator + invariants test:
  - Each run_id must have RUN_START and RUN_END
  - METRIC must exist if n_steps > 0
- Add explicit allowlist for scenario `config_overrides`
- Add CI regression gate comparing scenario metrics/boundaries across commits
- Integrate real model execution under a well-defined hook with bounded runtime

## 6. Traceability Links
- Threat Model: `docs/threat_model.md`
- Non-Goals: `docs/non_goals.md` (planned)
- Evaluation Protocol: (planned in docs + runner flags)
